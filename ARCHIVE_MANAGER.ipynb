{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352aa871-7871-42a6-affe-4b984d13a92e",
   "metadata": {},
   "source": [
    "## Chargement des données depuis HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fc38f6f-4824-4064-a46f-d53dc319644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|_c0|    Name_Player|\n",
      "+---+---------------+\n",
      "|  0|Hernán Galíndez|\n",
      "|  1|   Félix Torres|\n",
      "|  2| Piero Hincapié|\n",
      "|  3|Robert Arboleda|\n",
      "|  4| José Cifuentes|\n",
      "+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Name_Player: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.appName(\"Data Exploration\").getOrCreate()\n",
    "\n",
    "# Lire le fichier CSV depuis HDFS\n",
    "df = spark.read.csv(\"hdfs://192.168.111.130:9000/ilyas/hadoop/archives/listplayers.csv\", header=True)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "df.show(5)\n",
    "\n",
    "# Afficher la structure des colonnes du DataFrame\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e324884e-5e66-43e7-848f-5ae6653b7061",
   "metadata": {},
   "source": [
    "## Préparation des données (Création de colonnes supplémentaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19a6542e-e3c6-4372-a65a-2b47a8ac6320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------+-------------+\n",
      "|_c0|    Name_Player|file_size|archive_label|\n",
      "+---+---------------+---------+-------------+\n",
      "|  0|Hernán Galíndez|        0|            1|\n",
      "|  1|   Félix Torres|      100|            0|\n",
      "|  2| Piero Hincapié|      200|            1|\n",
      "|  3|Robert Arboleda|      300|            0|\n",
      "|  4| José Cifuentes|      400|            1|\n",
      "+---+---------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Convertir la colonne '_c0' en entier (car elle est en string actuellement)\n",
    "df = df.withColumn(\"_c0\", col(\"_c0\").cast(\"int\"))\n",
    "\n",
    "# Créer une colonne \"file_size\" (valeurs fictives en multipliant par 100)\n",
    "df = df.withColumn(\"file_size\", col(\"_c0\") * 100)\n",
    "\n",
    "# Créer une colonne \"archive_label\" en fonction de l'index (_c0 pair = 1, sinon 0)\n",
    "df = df.withColumn(\"archive_label\", when(df['_c0'] % 2 == 0, 1).otherwise(0))\n",
    "\n",
    "# Afficher les premières lignes pour vérifier les nouvelles colonnes\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f4ad3-4694-4d97-b0ac-86bc896c7379",
   "metadata": {},
   "source": [
    "## Modélisation avec Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdf8f5b7-8c1e-4707-a1f6-93fcec898058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------+\n",
      "| features|archive_label|prediction|\n",
      "+---------+-------------+----------+\n",
      "|  [600.0]|            1|       1.0|\n",
      "| [2400.0]|            1|       0.0|\n",
      "| [2500.0]|            0|       0.0|\n",
      "| [2900.0]|            0|       0.0|\n",
      "| [3600.0]|            1|       0.0|\n",
      "| [4700.0]|            0|       0.0|\n",
      "| [5000.0]|            1|       0.0|\n",
      "| [6700.0]|            0|       0.0|\n",
      "| [9200.0]|            1|       0.0|\n",
      "| [9500.0]|            0|       0.0|\n",
      "| [9800.0]|            1|       0.0|\n",
      "|[11200.0]|            1|       0.0|\n",
      "|[11600.0]|            1|       0.0|\n",
      "|[11900.0]|            0|       0.0|\n",
      "|[12200.0]|            1|       0.0|\n",
      "|[12400.0]|            1|       0.0|\n",
      "|[12600.0]|            1|       0.0|\n",
      "|[12900.0]|            0|       0.0|\n",
      "|[13300.0]|            0|       0.0|\n",
      "|[13500.0]|            0|       0.0|\n",
      "+---------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Sélectionner les colonnes \"file_size\" comme features pour le modèle\n",
    "assembler = VectorAssembler(inputCols=[\"file_size\"], outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Diviser les données en train et test\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=12345)\n",
    "\n",
    "# Créer le modèle de régression logistique\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"archive_label\")\n",
    "model = lr.fit(train)\n",
    "\n",
    "# Tester le modèle\n",
    "predictions = model.transform(test)\n",
    "predictions.select(\"features\", \"archive_label\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee7c7b-9a00-4fb1-8450-a5bedf2b0744",
   "metadata": {},
   "source": [
    "## Modélisation avec Forêt Aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "529969b1-566e-48b6-a122-506dfb5bc424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude du modèle de forêt aléatoire : 0.4129032258064516\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Créer et entraîner un modèle de forêt aléatoire\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"archive_label\", numTrees=10)\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "# Tester le modèle\n",
    "rf_predictions = rf_model.transform(test)\n",
    "\n",
    "# Évaluer l'exactitude\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"archive_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Exactitude du modèle de forêt aléatoire : {rf_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7d0fd-7be1-4c71-bc59-0223f8d3093a",
   "metadata": {},
   "source": [
    "## Modélisation avec Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fdca2db-be4a-427b-8310-2caba3ed68ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude du modèle Gradient Boosting : 0.3548387096774194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Gradient Boosted Trees Classifier\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"archive_label\", maxIter=10)\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "# Tester le modèle\n",
    "gbt_predictions = gbt_model.transform(test)\n",
    "\n",
    "# Évaluer l'exactitude du modèle Gradient Boosting\n",
    "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"Exactitude du modèle Gradient Boosting : {gbt_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7bc33-335b-404b-84ef-f4e0aba5e686",
   "metadata": {},
   "source": [
    "## Évaluation de la précision et du rappel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0ded765-1d36-42a3-ac33-b7c19f1c8d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle : 0.35816518580119927\n",
      "Rappel du modèle : 0.3548387096774194\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la précision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"archive_label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = precision_evaluator.evaluate(gbt_predictions)\n",
    "print(f\"Précision du modèle : {precision}\")\n",
    "\n",
    "# Calcul du rappel\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"archive_label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = recall_evaluator.evaluate(gbt_predictions)\n",
    "print(f\"Rappel du modèle : {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a8367-ef52-48da-92e1-812db2ab1a8c",
   "metadata": {},
   "source": [
    "## Filtrer et afficher les fichiers à archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d29bd5c0-7051-4f84-80cc-bf17c966cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Name_Player|file_size|\n",
      "+--------------------+---------+\n",
      "|      Jackson Porozo|     2400|\n",
      "|     Kevin Rodríguez|     2500|\n",
      "|Virgil van Dijk (...|     2900|\n",
      "|     Steven Berghuis|     3600|\n",
      "|     Denzel Dumfries|     4700|\n",
      "|         Xavi Simons|     5000|\n",
      "|     Boualem Khoukhi|     6700|\n",
      "|         Tom Lockyer|    19700|\n",
      "|         Mark Harris|    19900|\n",
      "|         Ben Cabango|    20400|\n",
      "|     Leandro Paredes|    21100|\n",
      "|     Guido Rodríguez|    22400|\n",
      "|    Nicolás Otamendi|    22500|\n",
      "|    Lautaro Martínez|    22800|\n",
      "|   Emiliano Martínez|    22900|\n",
      "|      Enzo Fernández|    23000|\n",
      "|       Johan Vásquez|    23700|\n",
      "|        Raúl Jiménez|    24100|\n",
      "|  Rogelio Funes Mori|    24300|\n",
      "|          Matty Cash|    26000|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les fichiers à archiver (prediction == 1.0)\n",
    "files_to_archive = gbt_predictions.filter(gbt_predictions.prediction == 1.0)\n",
    "\n",
    "# Afficher les fichiers qui devraient être archivés\n",
    "files_to_archive.select(\"Name_Player\", \"file_size\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650f55f-fbf5-4e9c-aa45-9913db8d7f09",
   "metadata": {},
   "source": [
    "## Écriture des fichiers compressés dans HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bec3ad8-d0f9-46b4-89e1-03302cdfecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler l'archivage ou la compression avec l'option d'écrasement (overwrite)\n",
    "files_to_archive.write.mode(\"overwrite\").parquet(\"hdfs://192.168.111.130:9000/ilyas/hadoop/archives_compressed/\", compression='snappy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c8d37-7ae4-4a82-a55a-e05a0db2fae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
